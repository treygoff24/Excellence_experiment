Ticket 112 — Local engine (preferred): Ollama HTTP client

Summary
- Implemented `backends/local/ollama_client.py` providing a synchronous HTTP client for Ollama.
- Supports both message-based (`/api/chat`) and prompt-based (`/api/generate`) calls.
- Parameter mapping: temperature, top_p, top_k, max_new_tokens→num_predict, stop, seed (via `options`).
- Added `health_check()` against `/api/tags` with actionable errors for server not running or missing model.
- Added package stub `backends/local/__init__.py`.

Usage Snippets
- Import check:
  - `python -c "from backends.local.ollama_client import OllamaClient; print('OK')"`
- One-liner sanity (replace <model>):
  - `python - <<<'from backends.local.ollama_client import OllamaClient as C; print(C().generate(prompt="Hello", model="<model>")["text"][:10])'`
- Health check (optional):
  - `python - <<<'from backends.local.ollama_client import OllamaClient; print(OllamaClient().health_check())'`

Notes
- Timeouts set (connect=5s, read up to 600s). Clear errors with remediation: `ollama serve`, `ollama pull <model>`.
- Usage fields populated when available (prompt_eval_count, eval_count).

