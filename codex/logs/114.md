# Ticket 114 — Local batch executor (JSONL in → JSONL out)

- Added `backends/local/local_batch.py` implementing a local work queue that:
  - Reads per-part JSONL inputs produced by `scripts.build_batches`.
  - Executes requests against a local engine (`ollama` or `llama_cpp`) with a bounded thread pool (hard‑capped at 2 concurrent requests by default).
  - Preserves input order in outputs; writes Fireworks‑compatible per‑part results to `<results_dir>/t<T>_<cond>_pXX/results.jsonl` with `{custom_id, response.body.choices[0].message.content, finish_reason, usage?, request_id, latency_s}`.
  - Idempotent resume: if output count equals input count, the part is skipped.
  - Writes a per‑part `errors.jsonl` and a lightweight `state.json` (counts, concurrency, temperature, condition).
- Exposed helpers mirroring the Fireworks executor surface:
  - `split_jsonl(...)` to shard inputs when needed.
  - `upload_datasets(...)` no‑op passthrough for local (returns paths).
  - `create_queue(...)` returns `LocalQueueManager` with `add_job(...)` and `run_queue(results_dir)`.

Acceptance notes:
- Outputs mimic the schema expected by `fireworks.parse_results`, enabling downstream parsing without changes.
- Concurrency defaults to ≤2; stable ordering is enforced per input order.
- Atomic writes prevent partial line corruption; resume skips already completed parts.
