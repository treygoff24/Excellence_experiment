Ticket 113 â€” Local engine (alternative): llama.cpp (llama-cpp-python)

Date: 2025-09-09

Summary
- Implemented `LlamaCppClient` in `backends/local/llama_cpp_client.py` providing in-process local inference.
- Added GGUF model support with configurable GPU offloading via `n_gpu_layers` parameter.
- Supports both OpenAI-style messages and raw prompt generation modes.
- Includes VRAM pressure warnings for large models (>12GB on 16GB systems).
- Implements `InferenceClient` protocol with standardized response format.

Implementation Details
- **Model Loading**: Uses `llama_cpp.Llama` with configurable context size (default 4096) and GPU layers.
- **Generation Modes**: 
  - Messages: `create_chat_completion()` for structured conversations
  - Prompt: Direct `__call__()` for raw text generation
- **Parameter Mapping**: Maps common parameters (temperature, top_p, top_k, max_new_tokens, stop sequences)
- **Error Handling**: Graceful fallback with error responses; specific CUDA/cuBLAS mismatch detection
- **Performance**: Full GPU offloading with `n_gpu_layers=-1` when VRAM permits

Safety Features
- Model file existence validation at initialization
- VRAM usage warnings for models >12GB (likely 13B Q6_K or larger)
- CUDA wheel mismatch detection with remediation instructions
- Request ID tracking and latency measurement

Configuration Options
- `model_path`: Path to GGUF model file (required)
- `n_ctx`: Context window size (default 4096, suitable for RTX 5080 16GB)
- `n_gpu_layers`: GPU offload layers (-1 for all layers)
- `n_threads`: CPU thread count (None for auto-detection)
- `verbose`: Enable llama.cpp debug output

Dependencies
- Requires `llama-cpp-python` with CUDA wheels for GPU acceleration
- Compatible with GGUF model format
- No daemon or HTTP server required (runs in-process)

Validation
- Supports swapping with Ollama via config-only changes
- Returns standardized response format matching `InferenceClient` protocol
- Provides deterministic generation with temperature=0.0
- VRAM usage will be visible in `nvidia-smi` during generation

Notes
- Designed for RTX 5080 (16GB VRAM) with safe defaults
- Recommends Q4_K_M quantization for throughput, Q6_K for quality on 16GB systems
- In-process execution provides lower overhead compared to HTTP-based solutions
- Model file validation prevents runtime errors from missing files