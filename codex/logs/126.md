# Log 126 — Windows Local Port Completion

## Summary
- Implemented local backend parity: manifest updates for local queues, llama.cpp concurrency clamp, token estimation fallback, and optional NVML telemetry.
- Added `scripts/estimate_tokens.py`, telemetry helpers, schema/config wiring, and ensured sample configs surface new knobs.
- Hardened CLI entry points with `multiprocessing.freeze_support()` and introduced Windows smoke CI workflow.
- Refreshed Windows documentation (installation, telemetry, troubleshooting) and README status to reflect full support.

## Files Touched
- `backends/local/local_batch.py`, `scripts/run_all.py`, `scripts/estimate_tokens.py`, `telemetry/nvml.py`
- `config/schema.py`, `config/eval_config.local*.yaml`
- `.github/workflows/windows-smoke.yml`
- `scripts/*.py` entrypoints (freeze_support injections)
- `docs/windows.md`, `docs/performance.md`, `docs/troubleshooting_windows_local.md`, `README.md`

## Validation
- `python -c "from config.schema import load_config; load_config('config/eval_config.local.yaml')"`
- `python -c "from config.schema import load_config; load_config('config/eval_config.local.llamacpp.yaml')"`
- `python -m scripts.run_all --config config/eval_config.local.yaml --dry_run --limit_items 10 --parts_per_dataset 2`
  - Dry run completed; stats/power analysis steps failed (expected) due to missing SciPy in local environment. Core orchestration, parsing, scoring, and cost stages succeeded.
- `python -m scripts.run_all --config config/eval_config.local.llamacpp.yaml --plan_only`
- Spot checks: `python - <<'PY'` harness to confirm llama.cpp concurrency clamp (forced to 1) and estimator helpers behave as expected.

## Follow-ups
- Full hardware validation on Windows with Ollama and llama.cpp backends is still required (run commands in docs §4.7 once GPUs available).
- Windows CI workflow should be monitored on the next PR to ensure the dry-run completes within time limits.
