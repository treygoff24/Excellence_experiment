backend: local
local_engine: llama_cpp
# Path to GGUF model on Windows (use double backslashes or forward slashes)
model_id: "llama3.1-8b-instruct-q4_k_m.gguf"
local_model: "C:/models/llama3.1-8b-instruct-q4_k_m.gguf"
tokenizer: "llama-3"
# Optional: record GPU telemetry (NVML / pynvml required)
enable_local_telemetry: false

# Optional llama.cpp tuning (overrides defaults when set)
# local_n_ctx: 4096
# local_n_gpu_layers: -1

temps: [0.0]
samples_per_item:
  "0.0": 1
max_new_tokens:
  closed_book: 1024
  open_book: 1024
top_p: 1.0
top_k: 50
stop: []

# llama.cpp is forced to single-worker execution for stability
max_concurrent_requests: 1

paths:
  raw_dir: "data/raw"
  prepared_dir: "data/prepared"
  batch_inputs_dir: "data/batch_inputs"
  results_dir: "results"
  reports_dir: "reports"
  run_manifest: "results/run_manifest.json"
  experiments_dir: "experiments"

prompt_sets:
  default:
    control: config/prompts/control_system.txt
    treatment: config/prompts/treatment_system.txt
default_prompt_set: default
